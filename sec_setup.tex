
% === DATA ==============

\paragraph{Data.}
We exploit user-generated language from an online forum of football fans,
namely, the r/LiverpoolFC subreddit, one of the many communities hosted
by the Reddit platform.\footnote{\url{https://www.reddit.com}.}
%\marco{I would remove this link, it is useless} 
%\raq{We can consider this at the end.}
We focus on a short period of eight years, between 2011 and 2017. 
In order to enable a clearer observation of short-term meaning shift, we define two
non-consecutive time bins: the first one ($t_1$) contains data from
2011--2013 and the second one ($t_2$) from 2017.\footnote{These choices
  ensure that the samples in these two time bins are approximately of the same size -- see Table~\ref{tab:data}. The
  r/LiverpoolFC subreddit exists since 2009, but very little content
  was produced in 2009--2010.}
 We also use a large sample of community-independent language for the
initialization of the word vectors, namely, a random crawl from Reddit
in 2013.\footnote{We used the Python package Praw for downloading the data, \url{https://pypi.python.org/pypi/praw}.} Table~\ref{tab:data} shows the size of each sample.

%----- TABLE ----------------------------------------------------
\begin{table}[t]\small
\centering
\begin{tabular}{lccc}
\bf sample & \bf time bin & \bf million tokens \\
 \hline
\redd &  2013 & {\raise.17ex\hbox{$\scriptstyle\sim$}}900 \\
LiverpoolFC$_{13}$ & 2011--13 & ~ 8.5\\
LiverpoolFC$_{17}$ & 2017 & 11.9\\ \hline
\end{tabular}
\caption{Time bin and size of the datasets.}
\label{tab:data}
\end{table}
%----- END TABLE -----------------------------------------



%=== MODEL ==========

\paragraph{Model.}
%We adapt the methodology introduced by 
%, which in turn builds on the skip-gram architecture by \newcite{mikolov2013distributed}
In the method proposed by \newcite{kim2014temporal}, word
embeddings for the first time bin $t_1$ are initialized randomly; then,
given a sequence of time-related samples, embeddings for $t_i$
are initialized using the embeddings of $t_{i-1}$ and further
updated. 
%with the standard skip-gram architecture.
If at $t_i$ the word is used in the same contexts as in $t_{i-1}$, its embedding will only be marginally updated, whereas a major change in the context of use will lead to a stronger update of the embedding. The model makes embeddings across time bins directly comparable.

%Recall that we want to spot changes that occur between 2013 and 2017 (the latter included). Since training directly on LiverpoolFC$_{13}$ is not possible due to data sparseness, 
We implement the following steps:
First, we create  randomly initialized word embeddings with the large sample \redd, to obtain meaning
representations that are community-independent.
%\footnote{\redd\ embeddings are initialized randomly.} \marco{Useless footnote, we say about the random initialization before}
We then use these embeddings
to initialize those in LiverpoolFC$_{13}$, update the vectors on this
sample, and thus obtain embeddings for time $t_1$. This step
adapts the general embeddings from \redd\  to the
LiverpoolFC community. Finally, we
initialize the word embeddings for LiverpoolFC$_{17}$ with those of $t_1$, train on this sample and get embeddings for $t_2$.\footnote{We implement the model
using the \texttt{gensim} library.}   
%To obtain word embeddings for time $t_2$, we
%initialize the word embeddings for LiverpoolFC$_{17}$ with those of $t_1$, 
%and train skip-gram further on the
%LiverpoolFC$_{17}$ data.

%We define the vocabulary to be represented as the intersection of the vocabularies of the three samples (\redd, LiverpoolFC$_{13}$, LiverpoolFC$_{17}$) which results in a vocabulary of 157k words.

The vocabulary is defined as the intersection of the
vocabularies of the three samples (\redd, LiverpoolFC$_{13}$,
LiverpoolFC$_{17}$), and includes 157k words.
%\todo{What is the vocabulary size?} 
%\redd~embeddings are initialized randomly. 
For \redd, we include only words that occur at least 20 times in the
sample, so as to ensure meaningful representations for each word,
while for the other two samples we do not use any frequency
threshold: Since the embeddings used for the initialization of
LiverpoolFC$_{13}$ encode community-independent meanings, if a word doesn't occur in
LiverpoolFC$_{13}$ its representation will simply be as in \redd,
which reflects the idea that if a word is not used in a community, then its meaning is not altered within
that community. 
%Instead, if a word's meaning changes in the
%community, then we expect the word embedding to change accordingly
%after training on the community-specific data.
%Analogous reasoning applies to LiverpoolFC$_{17}$. 
We train with standard skip-gram parameters \cite{levy2015improving}: window 5, learning rate 0.01, embedding dimension 200, hierarchical softmax.


%===== EVALUATION DATASET ===============


\paragraph{Evaluation dataset.}

We create a small dataset of words to be annotated by members of the r/LiverpoolFC
subreddit, that is, community members with domain knowledge (needed for this task) but no linguistic background.
We initially leverage information about increase in frequency, which has been shown to positively correlate with meaning change \cite{wijaya2011understanding,kulkarni2015statistically}, and sample content words with a significant increase in relative frequency (at least two standard deviations above the mean) between $t_1$ and $t_2$. Frequency increase is not a necessary condition for meaning shift to take place; however, it is a reasonable starting point, as a random selection of words would contain very few positive examples. Our dataset is thus biased towards precision. 
This procedure yields $\sim$200 words. We then manually identified cases of semantic shift among these words, i.e., changes in the ontological type of what a word denotes (see examples in Section~\ref{sec:types}) by analyzing their contexts of use in the r/LiverpoolFC data. This resulted in 34 words. We added two types of confounders: 33 words with a significant increase in frequency but not marked as meaning shift candidates and 33 words with constant frequency between $t_1$ and $t_2$, included as a sanity check.\footnote{All words have absolute frequency in range [50--500].} 

%We then created an online survey, which we posted in the r/LiverpoolFC to recruit participants. 
The participants were shown the 100 words (in randomized order) together with randomly chosen contexts of usage from each time period ($\mu$=4.7 contexts per word) and, for simplicity, were asked to make a binary decision
about whether there was a change in meaning. However, semantic shift is arguably a graded notion, so we aggregate the annotations into a graded \emph{semantic shift index}, ranging from 0 (no shift) to 1 (shift) depending on how many subjects spotted semantic change.\footnote{The shift index is exclusively based on the judgements by the redditors, and does not consider the preliminary candidates selection done by us.}  Overall, 26 members of r/LiverpoolFC participated in the survey, and each word received on average 8.8 judgements. The final dataset includes 97 words.\footnote{Three words were discarded: `discord'  and `owls' due to the homonymy with proper names not detected during survey's implementation; `tracking' because the chosen examples clearly mislead the judgements of the redditors.} 
Further details are in the supplementary material.


%*** SHORT VERSION***

%\paragraph{Evaluation dataset.}  
%The dataset includes 97 words, which were annotated by 26 members of the r/LiverpoolFC
%subreddit, that is, community members with domain knowledge (needed for this task) but no linguistic background.
%For each word, subjects
%read contexts of use from the two time bins and, for simplicity, were
%asked to make a binary decision
%about whether there was a change in meaning. However, semantic shift
%is arguably a graded notion, so we aggregate the annotations into a
%graded \emph{semantic shift index}, ranging from 0
%(no shift) to 1 (shift) depending on how many subjects spotted
%semantic change. This index is shown in the y-axis of
%Figure~\ref{fig:shift-cosine}. Further methodological details regarding data
%collection are described in the supplementary material. 



%*** LONG VERSION ***

%\paragraph{Evaluation dataset.}\todo{M: I commented out the short version of the paragraph and put back the long/original one, with small additions - in red} 
%For evaluation and analysis, we create a small dataset of words to be annotated as positive or negative meaning shift examples by community members without linguistic background.\footnote{Domain knowledge is needed for this task.}
%We initially leverage information about increase in frequency, which has been shown to positively correlate with meaning change \cite{wijaya2011understanding,kulkarni2015statistically}, and sample words with a significant increase in relative frequency between $t_1$ and $t_2$ (an increase is considered significant if it is at least two standard deviations above the mean).\footnote{We consider   content words only, which we identify by using the external list of common words available at \url{https://www.wordfrequency.info/free.asp}} Frequency increase is not a necessary condition for meaning shift to take place; however, given the positive correlation mentioned above, it is a reasonable starting point, as a random selection of words would contain very few positive examples. Our dataset is thus biased towards precision. 
%This procedure yields $\sim$200 words. The first author of the paper went through the list of words to identify cases of potential meaning shift, based on the analysis of the contexts of use in the r/LiverpoolFC data. By semantic shift, we mean change in the ontological type of what a word denotes (see examples in Section \ref{sect:results}). We considered only new senses - i.e., not existing senses which increase in frequency- , both for monosemous and polysemous words. This resulted in 34 words. We added two types of confounders: 33 words with a significant increase in frequency but not marked as meaning shift candidates and 33 words with constant frequency between $t_1$ and $t_2$, included as a sanity check.\footnote{All words have absolute frequency in range [50--500].} 
%
%We then created an online survey, which we posted in the r/LiverpoolFC to recruit participants. The participants were shown the 100 words together with randomly chosen contexts of usage from each time period (1 to 5 contexts depending on the word - \textcolor{red}{mean number of contexts per word=4.7})). For feasibility, they were asked to label words as `shift' or `no shift', although semantic shift is better viewed as graded (see below). The order of presentation was randomized for each participant.\footnote{Survey's instructions are in the supplementary material.} Overall, 26 members of r/LiverpoolFC participated in the survey, and each word in the dataset received on average 8.8 judgements. The final dataset includes 97 words.\footnote{Three words were discarded: `discord'  and `owls' due to the homonymy with proper names not detected during survey's implementation; `tracking' because the chosen examples clearly mislead the judgements of the redditors.} Inter-annotator agreement, computed as Krippendorff's alpha, is $\alpha$ = 0.58, a relatively low value but common in semantic tasks \cite{artstein2008inter}.
%We use the annotations to define a gradable \emph{semantic shift index}, computed as the proportion of `shift' judgements a word received in the survey.\footnote{The shift index is exclusively based on the judgements by the redditors, and does not consider the preliminary candidates selection done by one of us.} The index ranges from 0 (no shift) to 1 (shift). \textcolor{red}{As expected, all words with no frequency increase in $t_2$ have a shift index lower than 0.5 (average=0.07 $\pm$ 0.12), which validates our data selection method. For words labeled by the first author as potential shift, the average shift index is 0.72 $\pm$ 0.15, for words which increase in frequency 0.15 $\pm$ 0.16.}
%
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
